{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "DSxzF7DzIexq",
    "outputId": "21006433-8d7c-4769-ff89-cb5ef30343b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (2020.1.8)\n",
      "Requirement already satisfied: boto3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (1.11.5)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from boto3->transformers) (0.3.1)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from boto3->transformers) (1.14.5)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.5->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.5->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "XrQ5g4ihIqCv",
    "outputId": "b88ef59f-fde3-4514-88a3-fcfb2d8a1d7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_Qu-T9JItjf"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "        model,\n",
    "        input_ids=None,\n",
    "        max_length=None,\n",
    "        do_sample=None,\n",
    "        num_beams=None,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        repetition_penalty=None,\n",
    "        bos_token_id=None,\n",
    "        pad_token_id=None,\n",
    "        eos_token_ids=None,\n",
    "        length_penalty=None,\n",
    "        num_return_sequences=None,\n",
    "):\n",
    "    max_length = config_max_length\n",
    "    do_sample = True\n",
    "    num_beams = config_num_beams\n",
    "    temperature = config_temperature\n",
    "    top_k = config_top_k\n",
    "    top_p = config_top_p\n",
    "    repetition_penalty = config_repetition_penalty\n",
    "    bos_token_id = config_bos_token_id\n",
    "    pad_token_id = config_pad_token_id\n",
    "    eos_token_ids = [0]\n",
    "    length_penalty = config_length_penalty\n",
    "    num_return_sequences = config_num_return_sequences\n",
    "\n",
    "    batch_size = input_ids.shape[0]\n",
    "    cur_len = input_ids.shape[1]\n",
    "    vocab_size = 50257\n",
    "\n",
    "    if num_return_sequences != 1:\n",
    "        # Expand input to num return sequences\n",
    "        input_ids = input_ids.unsqueeze(1).expand(batch_size, num_return_sequences, cur_len)\n",
    "        input_ids = input_ids.contiguous().view(\n",
    "            batch_size * num_return_sequences, cur_len\n",
    "        )  # (batch_size * num_return_sequences, cur_len)\n",
    "        effective_batch_size = batch_size * num_return_sequences\n",
    "    else:\n",
    "        effective_batch_size = batch_size\n",
    "\n",
    "    if num_beams > 1:\n",
    "        output = generate_beam_search(\n",
    "            model,\n",
    "            input_ids,\n",
    "            cur_len,\n",
    "            max_length,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_k,\n",
    "            top_p,\n",
    "            repetition_penalty,\n",
    "            pad_token_id,\n",
    "            eos_token_ids,\n",
    "            effective_batch_size,\n",
    "            length_penalty,\n",
    "            num_beams,\n",
    "            vocab_size,\n",
    "        )\n",
    "    else:\n",
    "        output = generate_no_beam_search(\n",
    "            model,\n",
    "            input_ids,\n",
    "            cur_len,\n",
    "            max_length,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_k,\n",
    "            top_p,\n",
    "            repetition_penalty,\n",
    "            pad_token_id,\n",
    "            eos_token_ids,\n",
    "            effective_batch_size,\n",
    "        )\n",
    "\n",
    "    if num_return_sequences != 1:\n",
    "        output = output.view(batch_size, num_return_sequences, -1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_beam_search(\n",
    "        model,\n",
    "        input_ids,\n",
    "        cur_len,\n",
    "        max_length,\n",
    "        do_sample,\n",
    "        temperature,\n",
    "        top_k,\n",
    "        top_p,\n",
    "        repetition_penalty,\n",
    "        pad_token_id,\n",
    "        eos_token_ids,\n",
    "        batch_size,\n",
    "        length_penalty,\n",
    "        num_beams,\n",
    "        vocab_size,\n",
    "):\n",
    "    input_ids = input_ids.unsqueeze(1).expand(batch_size, num_beams, cur_len)\n",
    "    input_ids = input_ids.contiguous().view(batch_size * num_beams, cur_len)  # (batch_size * num_beams, cur_len)\n",
    "\n",
    "    generated_hyps = [\n",
    "        BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=False) for _ in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    # scores for each sentence in the beam\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n",
    "\n",
    "    # cache compute states\n",
    "    past = None\n",
    "\n",
    "    # done sentences\n",
    "    done = [False for _ in range(batch_size)]\n",
    "\n",
    "    while cur_len < max_length:\n",
    "        model_inputs = prepare_inputs_for_generation(input_ids, past=past)\n",
    "        outputs = model(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\n",
    "        scores = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "        # if model has past, then set the past variable to speed up decoding\n",
    "        if do_output_past(outputs):\n",
    "            past = outputs[1]\n",
    "\n",
    "        # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
    "        if repetition_penalty != 1.0:\n",
    "            for i in range(batch_size * num_beams):\n",
    "                for previous_token in set(input_ids[i].tolist()):\n",
    "                    # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
    "                    if scores[i, previous_token] < 0:\n",
    "                        scores[i, previous_token] *= repetition_penalty\n",
    "                    else:\n",
    "                        scores[i, previous_token] /= repetition_penalty\n",
    "\n",
    "        if do_sample:\n",
    "            # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "            if temperature != 1.0:\n",
    "                scores = scores / temperature\n",
    "            # Top-p/top-k filtering\n",
    "            scores = top_k_top_p_filtering(\n",
    "                scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n",
    "            )  # (batch_size * num_beams, vocab_size)\n",
    "            # Sample 2 next words for each beam (so we have some spare tokens and match output of greedy beam search)\n",
    "            next_words = torch.multinomial(F.softmax(scores, dim=-1), num_samples=2)  # (batch_size * num_beams, 2)\n",
    "            # Compute next scores\n",
    "            _scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "            _scores = torch.gather(_scores, -1, next_words)  # (batch_size * num_beams, 2)\n",
    "            next_scores = _scores + beam_scores[:, None].expand_as(_scores)  # (batch_size * num_beams, 2)\n",
    "            # Match shape of greedy beam search\n",
    "            next_words = next_words.view(batch_size, 2 * num_beams)  # (batch_size, 2 * num_beams)\n",
    "            next_scores = next_scores.view(batch_size, 2 * num_beams)  # (batch_size, 2 * num_beams)\n",
    "        else:\n",
    "            # do greedy beam search\n",
    "            scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "            assert scores.size() == (batch_size * num_beams, vocab_size)\n",
    "            # Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)\n",
    "            _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
    "            # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n",
    "            _scores = _scores.view(batch_size, num_beams * vocab_size)  # (batch_size, num_beams * vocab_size)\n",
    "            next_scores, next_words = torch.topk(_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        assert next_scores.size() == next_words.size() == (batch_size, 2 * num_beams)\n",
    "\n",
    "        # next batch beam content\n",
    "        # list of (batch_size * num_beams) tuple(next hypothesis score, next word, current position in the batch)\n",
    "        next_batch_beam = []\n",
    "\n",
    "        # for each sentence\n",
    "        for batch_ex in range(batch_size):\n",
    "\n",
    "            # if we are done with this sentence\n",
    "            done[batch_ex] = done[batch_ex] or generated_hyps[batch_ex].is_done(next_scores[batch_ex].max().item())\n",
    "            if done[batch_ex]:\n",
    "                next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n",
    "                continue\n",
    "\n",
    "            # next sentence beam content\n",
    "            next_sent_beam = []\n",
    "\n",
    "            # next words for this sentence\n",
    "            for idx, score in zip(next_words[batch_ex], next_scores[batch_ex]):\n",
    "\n",
    "                # get beam and word IDs\n",
    "                beam_id = idx // vocab_size\n",
    "                word_id = idx % vocab_size\n",
    "\n",
    "                # end of sentence, or next word\n",
    "                if word_id.item() in eos_token_ids or cur_len + 1 == max_length:\n",
    "                    generated_hyps[batch_ex].add(\n",
    "                        input_ids[batch_ex * num_beams + beam_id, :cur_len].clone(), score.item()\n",
    "                    )\n",
    "                else:\n",
    "                    next_sent_beam.append((score, word_id, batch_ex * num_beams + beam_id))\n",
    "\n",
    "                # the beam for next step is full\n",
    "                if len(next_sent_beam) == num_beams:\n",
    "                    break\n",
    "\n",
    "            # update next beam content\n",
    "            assert len(next_sent_beam) == 0 if cur_len + 1 == max_length else num_beams\n",
    "            if len(next_sent_beam) == 0:\n",
    "                next_sent_beam = [(0, pad_token_id, 0)] * num_beams  # pad the batch\n",
    "            next_batch_beam.extend(next_sent_beam)\n",
    "            assert len(next_batch_beam) == num_beams * (batch_ex + 1)\n",
    "\n",
    "        # sanity check / prepare next batch\n",
    "        assert len(next_batch_beam) == batch_size * num_beams\n",
    "        beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "        beam_words = input_ids.new([x[1] for x in next_batch_beam])\n",
    "        beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n",
    "\n",
    "        # re-order batch\n",
    "        input_ids = input_ids[beam_idx, :]\n",
    "        input_ids = torch.cat([input_ids, beam_words.unsqueeze(1)], dim=-1)\n",
    "\n",
    "        # re-order internal states\n",
    "        if past:\n",
    "            reordered_past = []\n",
    "            for layer_past in past:\n",
    "                # get the correct batch idx from layer past batch dim\n",
    "                # batch dim of `past` and `mems` is at 2nd position\n",
    "                reordered_layer_past = [layer_past[:, i].unsqueeze(1).clone().detach() for i in beam_idx]\n",
    "                reordered_layer_past = torch.cat(reordered_layer_past, dim=1)\n",
    "                # check that shape matches\n",
    "                assert reordered_layer_past.shape == layer_past.shape\n",
    "                reordered_past.append(reordered_layer_past)\n",
    "            past = tuple(reordered_past)\n",
    "\n",
    "        # update current length\n",
    "        cur_len = cur_len + 1\n",
    "\n",
    "        # stop when we are done with each sentence\n",
    "        if all(done):\n",
    "            break\n",
    "\n",
    "    tgt_len = input_ids.new(batch_size)\n",
    "    best = []\n",
    "\n",
    "    for i, hypotheses in enumerate(generated_hyps):\n",
    "        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
    "        tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
    "        best.append(best_hyp)\n",
    "\n",
    "    # generate target batch\n",
    "    decoded = input_ids.new(batch_size, tgt_len.max().item()).fill_(pad_token_id)\n",
    "    for i, hypo in enumerate(best):\n",
    "        decoded[i, : tgt_len[i] - 1] = hypo\n",
    "        decoded[i, tgt_len[i] - 1] = eos_token_ids[0]\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def generate_no_beam_search(\n",
    "        model,\n",
    "        input_ids,\n",
    "        cur_len,\n",
    "        max_length,\n",
    "        do_sample,\n",
    "        temperature,\n",
    "        top_k,\n",
    "        top_p,\n",
    "        repetition_penalty,\n",
    "        pad_token_id,\n",
    "        eos_token_ids,\n",
    "        batch_size,\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamHypotheses(object):\n",
    "    def __init__(self, n_hyp, max_length, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length - 1  # ignoring bos_token\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_hyp = n_hyp\n",
    "        self.hyp = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.hyp)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
    "        if len(self) < self.n_hyp or score > self.worst_score:\n",
    "            self.hyp.append((score, hyp))\n",
    "            if len(self) > self.n_hyp:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
    "                del self.hyp[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs):\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated\n",
    "        can become better than the worst one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "        if len(self) < self.n_hyp:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            return self.worst_score >= best_sum_logprobs / self.max_length ** self.length_penalty\n",
    "\n",
    "\n",
    "def prepare_inputs_for_generation(input_ids, **kwargs):\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "\n",
    "def do_output_past(outputs):\n",
    "    has_output_past = True\n",
    "    has_mem_len = False\n",
    "\n",
    "    if has_output_past and not has_mem_len and len(outputs) > 1:\n",
    "        return True\n",
    "    # elif has_mem_len and self.config.mem_len > 0 and len(outputs) > 1:\n",
    "    #     return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\"), min_tokens_to_keep=1):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size, vocabulary size)\n",
    "            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "            Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "dv7vPQYQIyYs",
    "outputId": "310aa9e5-e28e-4725-e2b3-b166ab3e743c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-vocab.json from cache at /Users/christopher/.cache/torch/transformers/eb2d31fb18c927045d8ccc07cace8bf1c10458bf171a5ad4cb1cbe0b75773425.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-merges.txt from cache at /Users/christopher/.cache/torch/transformers/18d7ac53606f670f979f24836b00f5dfee1c58d79bdbcc58411265f194d88ac0.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VdWe7rkzOl1D",
    "outputId": "5fd17ede-2391-4354-ac82-c0bfcaaad5ee"
   },
   "outputs": [],
   "source": [
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rOfzAL3nFiG"
   },
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ns54MSCEJleA"
   },
   "outputs": [],
   "source": [
    "config_max_length = 100\n",
    "config_do_sample = False\n",
    "config_num_beams=5\n",
    "config_temperature=0.7\n",
    "config_top_k=50\n",
    "config_top_p=1.0\n",
    "config_repetition_penalty=1.5\n",
    "config_bos_token_id=0\n",
    "config_pad_token_id=0\n",
    "config_eos_token_ids=0\n",
    "config_length_penalty=1.0\n",
    "config_num_return_sequences=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "BaiPX_aGJ8ur",
    "outputId": "24ec9876-a08e-4254-a87b-472f1a608455"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "text = \"Autism is a developm\"\n",
    "# input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0).to('cuda')\n",
    "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    outputs = generate(\n",
    "        model=model,\n",
    "        input_ids=input_ids,\n",
    "    )\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "if config_num_return_sequences == 1:\n",
    "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "else:\n",
    "  for i in range(config_num_return_sequences):\n",
    "    print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UHOxDU489jha",
    "outputId": "19e447cf-c4e7-4118-f08a-d302db2f3375"
   },
   "outputs": [],
   "source": [
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nlm1P5Pr9m1a"
   },
   "outputs": [],
   "source": [
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N02OtMBJ7uft"
   },
   "outputs": [],
   "source": [
    "def gen_text_from_text(in_text):\n",
    "  # print(Style.RESET_ALL)\n",
    "  # print(Fore.GREEN, 'INFO: text generation for snippet \"{}\":'.format(in_text))\n",
    "  start_time = time.time()\n",
    "  text = in_text\n",
    "  input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0).to('cuda')\n",
    "  with torch.no_grad():\n",
    "      outputs = generate(\n",
    "          model=model,\n",
    "          input_ids=input_ids,\n",
    "      )\n",
    "  elapsed_time = time.time() - start_time\n",
    "  print(Fore.GREEN, 'INFO: Finished Generation in {:4.2f} seconds. Result:'.format(elapsed_time))\n",
    "  # print(Style.RESET_ALL)\n",
    "\n",
    "  if config_num_return_sequences == 1:\n",
    "    # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    # print(Fore.RED, 'INFO: END OF RESULT')\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  else:\n",
    "    for i in range(config_num_return_sequences):\n",
    "      print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))\n",
    "      # print(Fore.RED, 'INFO: END OF RESULT')\n",
    "      # print('RESULT IS CURRENTLY NOT BEING RETURNED BY METHOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Nmpq1dNQK4p"
   },
   "outputs": [],
   "source": [
    "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "# !pip install gputil\n",
    "# !pip install psutil\n",
    "# !pip install humanize\n",
    "# import psutil\n",
    "# import humanize\n",
    "# import os\n",
    "# import GPUtil as GPU\n",
    "# GPUs = GPU.getGPUs()\n",
    "# # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "# gpu = GPUs[0]\n",
    "# def printm():\n",
    "#  process = psutil.Process(os.getpid())\n",
    "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "# printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xDKFHTSAfTD5",
    "outputId": "732feec5-8b7a-49d7-e963-b82784d483c8"
   },
   "outputs": [],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "138xsjmfngqk",
    "outputId": "0a5900cb-42b0-4646-e8f0-55695dc826ab"
   },
   "outputs": [],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "GijYq5KqhcMc",
    "outputId": "7a56ac4b-660e-4052-ea64-72fec8fcf97f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zH5iFp_n2W0H"
   },
   "outputs": [],
   "source": [
    "def gen_text_from_file(path, in_len=40, out_len=100, temp=1.0, reppen=1.0):\n",
    "  import json\n",
    "  results = []\n",
    "  with open(path, 'r') as r_file:\n",
    "    input_text = None\n",
    "    new_lines = []\n",
    "    for i, line in enumerate(r_file):\n",
    "      tmp = json.loads(line)\n",
    "      input_text = tmp['text'].split('\\n\\n', 1)[1][:in_len]\n",
    "      result = gen_text_from_text(input_text)\n",
    "      results.append(result)\n",
    "      json_tmp = json.dumps(tmp)\n",
    "      new_lines.append(json_tmp)\n",
    "    \n",
    "    with open(path[:-10] + 'gen_examples/' + path[-7:], 'w') as w_file:\n",
    "      for i, line in enumerate(new_lines):\n",
    "        new_tmp = json.loads(line)\n",
    "        # new_tmp['id']\n",
    "        # new_tmp['title']\n",
    "        # new_tmp['url']\n",
    "        new_tmp['text'] = results[i]\n",
    "        w_file.write(json.dumps(new_tmp) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "Z6lIDtUu49l9",
    "outputId": "ba084980-0f93-4f2d-fa00-cf5380fe1e6e"
   },
   "outputs": [],
   "source": [
    "gen_text_from_file('/content/drive/My Drive/AA/wiki_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBKeABsJozcO"
   },
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84dfkxvC5ABZ"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize('Andorra (, ; ), offi')\n",
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kiX1gYTy6hWQ",
    "outputId": "373afe0d-3007-440a-d8cc-c256acc72786"
   },
   "outputs": [],
   "source": [
    "!ls /root/.cache/torch/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "If7YdeIsp3qg"
   },
   "outputs": [],
   "source": [
    "# weights are cb17d4cd37fbb1f4c8a40b8419e3bdeafe212f2661ef7d0c8b5dd1bdb8f874c1.7013c97af6a899790dfb77a9d9b5230945da1f17f0b79da57a8941eaa3be03ca"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "improved_gpt2_generator.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
